# Neural Alchemy

**Neural Alchemy** is an open-source organization building **security frameworks, tooling, and infrastructure for AI & LLM systems**.  
We focus on making modern AI applications **secure by design**, testable before deployment, and resilient against real-world attacks.

Our projects are **framework-agnostic**, production-oriented, and designed to integrate cleanly into existing AI stacks.

---

## Mission

Enable developers and security teams to **build, test, and deploy AI systems safely** by providing:
- Offensive security tooling for LLMs
- Defensive frameworks for runtime protection
- Research-backed, practical security primitives

---

## Core Security Frameworks

### PromptXploit  
*A comprehensive security testing framework for LLM applications.*

Test AI systems for vulnerabilities **before deployment** using an extensive and adaptive attack arsenal.

**Key capabilities**
- ðŸŽ¯ 147 attack vectors across 17 categories
- ðŸ§  AI-powered adaptive mode that learns defenses and crafts new attacks
- ðŸ” Intelligence-based reconnaissance for novel attack discovery
- ðŸ“Š Structured JSON reports for detailed vulnerability analysis
- ðŸ”Œ Framework-agnostic â€” works with any LLM stack

Use PromptXploit to red-team prompt pipelines, agents, and AI-driven workflows.

---

### PromptShield  
*A lightweight runtime security framework for protecting AI applications.*

PromptShield defends against common and advanced LLM-specific attacks with minimal overhead.

**Protects against**
- ðŸš« Prompt injection
- ðŸ”“ Jailbreak attempts
- ðŸ“¤ System prompt extraction
- ðŸ” PII leakage
- ðŸŽ­ Dozens of known and generalized attack variants

**Key capabilities**
- âš¡ Ultra-fast: ~0.1ms (pattern mode), ~20â€“30ms (semantic mode)
- ðŸ”Œ Framework-agnostic: OpenAI, Anthropic, local models, and more
- ðŸŽ¯ Simple integration: usable in ~3 lines of code
- ðŸ›¡ï¸ Comprehensive coverage with semantic generalization

Designed for **production inference paths** where latency and reliability matter.

---

## Design Principles

- Security-first: Built with adversarial thinking from day one  
- LLM-agnostic: No vendor lock-in  
- Composable: Works with agents, pipelines, APIs, and tools  
- Practical: Focused on real attack surfaces, not theory alone  

---

## Ecosystem

Neural Alchemy projects are intended to be used:
- In CI/CD pipelines for AI security testing
- As runtime middleware for LLM APIs
- Alongside agent frameworks and orchestration layers
- In red-team and blue-team AI security workflows

---

## Neural Alchemy Labs

Experimental, research-driven work lives in **neural-alchemy-labs**.  
Labs projects may include:
- Novel attack research
- Prototype defenses
- Exploratory agent security models
- Early-stage tools and proofs-of-concept

> Labs projects are experimental and not guaranteed to be production-ready.

---

## Community & Contributions

We welcome:
- Security researchers
- AI engineers
- Red-team / blue-team practitioners
- Open-source contributors

Contributions, discussions, and issue reports help shape the future of AI security.

---

## License

All projects are released under open-source licenses unless stated otherwise.  
See individual repositories for details.
